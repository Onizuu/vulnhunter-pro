"""
Orchestrateur principal des scans
Coordonne tous les modules de scan et l'analyse IA
"""

import asyncio
import aiohttp
import time
import os  # Pour NIST_API_KEY
from datetime import datetime
from typing import Dict, List, Optional
from loguru import logger

from core.models import DonneesReconnaissance, Vulnerabilite, RapportScan
from integration_ia.openai_client import ClientOpenAI
from integration_ia.mistral_client import ClientMistral
from integration_ia.ia_client_fallback import ClientIAFallback
from core.validator import ValidateurVulnerabilites
from core.exploit_generator import GenerateurExploits
from modules.reconnaissance.subdomain_enum import EnumerateurSousdomaines
from modules.reconnaissance.port_scanner import ScannerPorts
from modules.reconnaissance.tech_detection import DetecteurTechnologies
from modules.reconnaissance.directory_fuzzer import FuzzerRepertoires
from modules.reconnaissance.parameter_discovery import DecouvreurParametres
from modules.reconnaissance.wayback_analyzer import WaybackAnalyzer
from modules.reconnaissance.github_recon import GitHubRecon
from modules.vulnerabilites.sql_injection import ScannerSQLInjection
from modules.vulnerabilites.xss_scanner import ScannerXSS
from modules.vulnerabilites.xxe_detector import DetecteurXXE
from modules.vulnerabilites.rce_finder import ChercheurRCE
from modules.vulnerabilites.idor_checker import VerificateurIDOR
from modules.vulnerabilites.cors_miscfg import AnalyseurCORS
from modules.vulnerabilites.header_analysis import AnalyseurHeaders
from modules.vulnerabilites.cve_scanner import ScannerCVE
from modules.vulnerabilites.config_analyzer import AnalyseurConfiguration
from modules.vulnerabilites.auth_bypass import TesteurAuthBypass
from modules.vulnerabilites.csrf_detector import DetecteurCSRF
from modules.vulnerabilites.file_upload_scanner import ScannerFileUpload
from modules.vulnerabilites.api_fuzzer import ApiFuzzer
from modules.vulnerabilites.lfi_scanner import LFIScanner  # ‚≠ê NOUVEAU: LFI Scanner AutoPWN
from modules.vulnerabilites.graphql_fuzzer import GraphQLFuzzer
# ‚≠ê NOUVEAUX MODULES v4.3 (comblent les lacunes critiques)
from modules.vulnerabilites.ssrf_detector import DetecteurSSRF
from modules.vulnerabilites.ssti_scanner import ScannerSSTI
from modules.vulnerabilites.nosql_injection import ScannerNoSQLInjection
from modules.vulnerabilites.deserialization_detector import DetecteurDeserialization
from modules.vulnerabilites.waf_detector import DetecteurWAF
# ‚≠ê NOUVEAUX MODULES v4.4 (couverture compl√®te)
from modules.vulnerabilites.prototype_pollution import DetecteurPrototypePollution
from modules.vulnerabilites.ldap_injection import DetecteurLDAPInjection
from modules.vulnerabilites.open_redirect import DetecteurOpenRedirect
from modules.vulnerabilites.clickjacking import DetecteurClickjacking
from modules.vulnerabilites.websocket_scanner import DetecteurWebSocket
from modules.vulnerabilites.race_conditions import DetecteurRaceConditions
from modules.vulnerabilites.business_logic import DetecteurBusinessLogic
from modules.intelligence.chain_builder import ConstructeurChaines
from modules.intelligence.ml_detector import DetecteurML
from modules.intelligence.risk_scorer import ScorerRisqueIntelligent
from modules.intelligence.ai_payload_generator import GenerateurPayloadsIA
from modules.intelligence.nist_cve_searcher import NISTCVESearcher  # ‚≠ê NOUVEAU: NIST CVE
from utilitaires.logger import ConfigurerLogger
from urllib.parse import urljoin


class MoteurScanIntelligent:
    """
    Moteur de scan principal qui coordonne toutes les phases
    d'analyse et utilise l'IA pour am√©liorer la d√©tection
    """

    def __init__(self, config: Dict):
        """
        Initialise le moteur de scan
        
        Args:
            config: Configuration du scan (cl√©s API, intensit√©, etc.)
        """
        self.config = config
        self.modules_cibles = config.get('modules_cibles', [])  # ‚≠ê NOUVEAU: Modules √† scanner (vide = tous)
        self.scan_type = config.get('scan_type', 'full')  # ‚≠ê NOUVEAU: 'full' ou 'specific_url'
        self.auth_config = config.get('auth', {})  # ‚≠ê NOUVEAU: Authentification
        self.ia_active = config.get('ia_active', True)
        self.callback_vulnerabilite = config.get('callback_vulnerabilite')  # ‚≠ê NOUVEAU: Callback pour le dashboard
        
        # ‚≠ê NOUVEAU: Contr√¥le de l'ex√©cution (Pause/Resume)
        self.pause_event = asyncio.Event()
        self.pause_event.set()  # Par d√©faut, le scan n'est pas en pause
        self.est_en_pause = False
        
        self.client_ia = None
        if self.ia_active:
            # ‚≠ê NOUVEAU: Syst√®me Ollama principal + Claude fallback (budget 5‚Ç¨ max)
            ollama_model = config.get('ollama_model', 'mistral:7b')
            claude_key = config.get('anthropic_api_key')
            budget_max = float(config.get('claude_budget_max', 5.0))  # 5‚Ç¨ par d√©faut
            
            # Utiliser le syst√®me de fallback intelligent
            self.client_ia = ClientIAFallback(
                ollama_model=ollama_model,
                claude_api_key=claude_key,
                budget_max=budget_max
            )
            
            if self.client_ia.disponible:
                logger.info(
                    f"ü§ñ IA configur√©e: Ollama principal ({ollama_model})"
                    + (f" + Claude fallback (budget: {budget_max}‚Ç¨)" if claude_key else " (Claude non configur√©)")
                )
            else:
                logger.warning("‚ö†Ô∏è  Ollama non disponible - Mode sans IA activ√©")
                logger.info("üí° Installez Ollama: brew install ollama && ollama pull mistral:7b")
        else:
            logger.info("ü§ñ IA d√©sactiv√©e - utilisation des payloads int√©gr√©s (scan plus rapide)")
        
        # ‚≠ê AM√âLIORATION: Validateur tr√®s permissif pour d√©tecter TOUTES les vuln√©rabilit√©s
        # Mode maximum: accepter toutes les vuln√©rabilit√©s d√©tect√©es
        self.validateur = ValidateurVulnerabilites(
            min_confirmations=2,  # ‚≠ê 2 confirmations requises pour filtrer drastiquement le bruit (182 -> ~20)
            tester_exploitation=True,  # ‚≠ê Tests d'exploitation r√©els activ√©s
            client_ia=self.client_ia,  # ‚≠ê IA pour g√©n√©ration d'exploits personnalis√©s
            mode_rapide=True  # ‚≠ê Toujours en mode rapide pour accepter plus
        )
        self.generateur_exploits = GenerateurExploits(self.client_ia)
        
        # Initialiser les modules de reconnaissance
        self.enum_sousdomaines = EnumerateurSousdomaines()
        self.scanner_ports = ScannerPorts()
        self.detecteur_tech = DetecteurTechnologies()
        self.fuzzer_rep = FuzzerRepertoires()
        self.decouvreur_params = DecouvreurParametres(self.auth_config)  # ‚≠ê NOUVEAU: D√©couverte automatique de param√®tres
        
        # Initialiser les scanners de vuln√©rabilit√©s
        self.scanner_sql = ScannerSQLInjection(self.client_ia, self.auth_config)
        self.scanner_xss = ScannerXSS(self.client_ia, self.auth_config)
        self.detecteur_xxe = DetecteurXXE()
        self.chercheur_rce = ChercheurRCE(self.client_ia)
        self.verif_idor = VerificateurIDOR()
        self.analyseur_cors = AnalyseurCORS()
        self.analyseur_headers = AnalyseurHeaders()
        self.scanner_cve = ScannerCVE(client_ia=self.client_ia)  # ‚≠ê IA pour exploits 0-day
        self.analyseur_config = AnalyseurConfiguration(self.client_ia)
        self.testeur_auth = TesteurAuthBypass(self.client_ia)
        self.detecteur_csrf = DetecteurCSRF()
        self.scanner_upload = ScannerFileUpload()  # ‚≠ê NOUVEAU: Scanner File Upload
        self.api_fuzzer = ApiFuzzer(self.client_ia, self.auth_config)  # ‚≠ê NOUVEAU: API Fuzzer
        self.graphql_fuzzer = GraphQLFuzzer(None, self.auth_config)  # ‚≠ê NOUVEAU: GraphQL Fuzzer (session initialis√©e plus tard)
        self.lfi_scanner = LFIScanner(self.auth_config)  # ‚≠ê NOUVEAU: LFI Scanner AutoPWN (50+ payloads)
        
        # ‚≠ê MODULES v4.3: Combler les lacunes critiques
        self.ssrf_detector = DetecteurSSRF(self.auth_config)  # ‚≠ê SSRF (OWASP Top 10)
        self.ssti_scanner = ScannerSSTI(self.auth_config)  # ‚≠ê SSTI (RCE critique)
        self.nosql_scanner = ScannerNoSQLInjection(self.auth_config)  # ‚≠ê NoSQL Injection
        self.deserialization_detector = DetecteurDeserialization(self.auth_config)  # ‚≠ê Deserialization (Java/Python/PHP)
        self.waf_detector = DetecteurWAF()  # ‚≠ê WAF Detection
        
        # ‚≠ê MODULES v4.4: Couverture compl√®te (100%)
        self.prototype_pollution_detector = DetecteurPrototypePollution(self.auth_config)  # ‚≠ê Prototype Pollution (Node.js)
        self.ldap_injection_detector = DetecteurLDAPInjection(self.auth_config)  # ‚≠ê LDAP Injection (Active Directory)
        self.open_redirect_detector = DetecteurOpenRedirect(self.auth_config)  # ‚≠ê Open Redirect (Phishing)
        self.clickjacking_detector = DetecteurClickjacking()  # ‚≠ê Clickjacking (X-Frame-Options)
        self.websocket_detector = DetecteurWebSocket(self.auth_config)  # ‚≠ê WebSocket Security
        self.race_conditions_detector = DetecteurRaceConditions(self.auth_config)  # ‚≠ê Race Conditions (TOCTOU)
        self.business_logic_detector = DetecteurBusinessLogic(self.auth_config)  # ‚≠ê Business Logic Flaws
        
        # Module d'intelligence
        self.constructeur_chaines = ConstructeurChaines(self.client_ia)
        self.detecteur_ml = DetecteurML()
        self.scorer_risque = ScorerRisqueIntelligent()
        self.generateur_payloads_ia = GenerateurPayloadsIA(self.client_ia)
        self.nist_cve = NISTCVESearcher(api_key=os.getenv('NIST_API_KEY'))  # ‚≠ê NOUVEAU: NIST CVE Database
        
        # Statistiques
        self.stats = {
            'requetes_totales': 0,
            'vulnerabilites_trouvees': 0,
            'faux_positifs_elimines': 0,
            'temps_par_phase': {}
        }
        
        logger.info("Moteur de scan initialis√© avec succ√®s")

    def pauser(self):
        """Met le scan en pause"""
        self.pause_event.clear()
        self.est_en_pause = True
        logger.info("‚è∏Ô∏è  Scan mis en pause")

    def reprendre(self):
        """Reprend le scan"""
        self.pause_event.set()
        self.est_en_pause = False
        logger.info("‚ñ∂Ô∏è  Scan repris")

    async def scanner_complet(self, url_cible: str) -> RapportScan:
        """
        Ex√©cute un scan complet sur la cible
        
        Args:
            url_cible: URL de la cible √† scanner
            
        Returns:
            RapportScan: Rapport complet avec toutes les vuln√©rabilit√©s
        """
        date_debut = datetime.now()
        logger.info(f"üéØ D√©marrage du scan complet sur : {url_cible}")
        
        try:
            # ‚≠ê Phase 0: Reconnaissance Passive (NOUVEAU)
            import os
            if os.getenv('ENABLE_PASSIVE_RECON', 'true').lower() == 'true':
                logger.info("üïµÔ∏è  Phase 0: Reconnaissance Passive...")
                passive_data = await self.phase_reconnaissance_passive(url_cible)
                
                # Ajouter les subdomains d√©couverts √† la reconnaissance active
                if passive_data.github_subdomains:
                    logger.info(
                        f"üìã {len(passive_data.github_subdomains)} subdomains GitHub ajout√©s"
                    )
            else:
                passive_data = None
                logger.info("‚è≠Ô∏è  Reconnaissance passive d√©sactiv√©e")
            
            # ‚≠ê Phase 0.5: D√©tection WAF (NOUVEAU v4.3)
            logger.info("üõ°Ô∏è  Phase 0.5: D√©tection de WAF...")
            waf_info = await self.waf_detector.detecter(url_cible)
            if waf_info and waf_info.get('waf_detected'):
                logger.warning(
                    f"‚ö†Ô∏è  WAF d√©tect√©: {waf_info['waf_type']} "
                    f"(confiance: {waf_info['confidence']}%)"
                )
                logger.info("üí° Suggestions de bypass:")
                for suggestion in waf_info.get('suggestions', [])[:3]:
                    logger.info(f"   - {suggestion}")
            else:
                logger.success("‚úÖ Aucun WAF d√©tect√© - scan optimal")
            
            # Phase 1: Reconnaissance
            if self.scan_type == 'specific_url':
                logger.info(f"üéØ Mode cibl√©: Scan uniquement sur {url_cible}")
                # Cr√©er des donn√©es de reconnaissance minimales
                donnees_recon = DonneesReconnaissance(url_cible=url_cible)
                donnees_recon.repertoires = []  # Pas de crawling
                donnees_recon.technologies = [] # On pourrait d√©tecter, mais on reste simple
                donnees_recon.ports_ouverts = []
                donnees_recon.sousdomaines = []
            else:
                logger.info("üì° Phase 1: Reconnaissance en cours...")
                donnees_recon = await self.phase_reconnaissance(url_cible)
            
            # Phase 2: D√©tection de vuln√©rabilit√©s
            logger.info("üîç Phase 2: D√©tection de vuln√©rabilit√©s...")
            vulnerabilites = await self.phase_detection_vulnerabilites(
                url_cible, donnees_recon
            )
            
            # Phase 3: Validation (√©liminer les faux positifs)
            logger.info("‚úÖ Phase 3: Validation des d√©couvertes...")
            vulns_validees = await self.phase_validation(vulnerabilites)
            
            # Phase 4: G√©n√©ration d'exploits
            logger.info("‚ö° Phase 4: G√©n√©ration d'exploits...")
            await self.phase_generation_exploits(vulns_validees)
            
            # Phase 5: Construction de cha√Ænes d'exploit
            logger.info("üîó Phase 5: Construction de cha√Ænes d'exploit...")
            chaines = await self.phase_chaines_exploit(vulns_validees)
            
            # Phase 6: √âvaluation du risque global
            logger.info("üìä Phase 6: √âvaluation du risque...")
            score_risque = self.calculer_score_risque_global(vulns_validees)
            
            date_fin = datetime.now()
            duree = (date_fin - date_debut).total_seconds()
            
            # ‚≠ê Calculer les statistiques par s√©v√©rit√©
            stats_severite = {}
            for vuln in vulns_validees:
                severite = vuln.severite
                stats_severite[severite] = stats_severite.get(severite, 0) + 1
            
            self.stats['par_severite'] = stats_severite
            self.stats['total_vulnerabilites'] = len(vulns_validees)
            
            # Cr√©er le rapport final
            rapport = RapportScan(
                url_cible=url_cible,
                date_debut=date_debut,
                date_fin=date_fin,
                duree=duree,
                vulnerabilites=vulns_validees,
                donnees_recon=donnees_recon,
                score_risque_global=score_risque,
                chaines_exploit=chaines,
                statistiques=self.stats
            )
            
            logger.success(
                f"‚ú® Scan termin√© ! {len(vulns_validees)} vuln√©rabilit√©s valid√©es "
                f"en {duree:.2f}s"
            )
            
            return rapport
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du scan: {str(e)}")
            raise


    async def phase_reconnaissance_passive(
        self, url_cible: str
    ) -> 'PassiveReconData':
        """
        Phase 0: Reconnaissance Passive
        Utilise sources publiques pour d√©couvrir assets sans √™tre d√©tect√©
        
        Args:
            url_cible: URL de la cible
            
        Returns:
            PassiveReconData: R√©sultats de la reconnaissance passive
        """
        from core.models import PassiveReconData, WaybackResult, GitHubAsset
        import os
        
        debut = time.time()
        logger.info("üïµÔ∏è  D√©but reconnaissance passive...")
        
        # Initialiser les modules
        wayback = WaybackAnalyzer()
        github_tokens = os.getenv('GITHUB_TOKENS', '').split(',')
        github_tokens = [t.strip() for t in github_tokens if t.strip()]
        github = GitHubRecon(github_tokens if github_tokens else None)
        
        # Extraire le domaine
        from urllib.parse import urlparse
        parsed = urlparse(url_cible)
        domain = parsed.netloc or parsed.path
        
        try:
            # Wayback Machine
            logger.info("üìö Wayback Machine: Analyse en cours...")
            wayback_urls = wayback.wayback_urls(domain, include_subdomains=False)
            wayback_robots = wayback.wayback_robots(domain)
            wayback_hidden = wayback.find_hidden_endpoints(domain)
            wayback_params = wayback.analyze_parameters(domain)
            
            wayback_result = WaybackResult(
                urls_discovered=wayback_urls,
                robots_paths=wayback_robots,
                hidden_endpoints=wayback_hidden,
                parameters=wayback_params,
                total_urls=len(wayback_urls)
            )
            
            # GitHub Recon
            github_subdomains = []
            github_creds = []
            github_keys = []
            
            if github_tokens:
                logger.info("üîç GitHub: Recherche d'assets...")
                github_subdomains = github.search_subdomains(domain, max_pages=3)
                github_creds = github.search_credentials(domain, max_pages=2)
                github_keys = github.search_api_keys(domain)
                
                # Convertir en GitHubAsset
                cred_assets = [
                    GitHubAsset(
                        type='credential',
                        value=cred['value'],
                        source=cred['source'],
                        repository=cred.get('repository', ''),
                        path=cred.get('path', ''),
                        severity='CRITICAL'
                    )
                    for cred in github_creds
                ]
                
                key_assets = [
                    GitHubAsset(
                        type='api_key',
                        value=key['key'],
                        source=key['source'],
                        repository=key.get('repository', ''),
                        severity='CRITICAL'
                    )
                    for key in github_keys
                ]
                
                github_creds = cred_assets
                github_keys = key_assets
            else:
                logger.warning("‚ö†Ô∏è  Pas de tokens GitHub - Reconnaissance GitHub skip")
            
            # Calculer totaux
            total_assets = (
                len(wayback_urls) + 
                len(wayback_robots) +
                len(github_subdomains) +
                len(github_creds) +
                len(github_keys)
            )
            
            duree = time.time() - debut
            
            passive_data = PassiveReconData(
                wayback_result=wayback_result,
                github_subdomains=github_subdomains,
                github_credentials=github_creds,
                github_api_keys=github_keys,
                total_assets_discovered=total_assets,
                execution_time=duree
            )
            
            logger.success(
                f"‚úÖ Passive Recon termin√©e: {total_assets} assets en {duree:.2f}s"
            )
            
            # Alerter si credentials trouv√©s
            if github_creds or github_keys:
                logger.critical(
                    f"üö® {len(github_creds + github_keys)} CREDENTIALS EXPOS√âS TROUV√âS!"
                )
            
            return passive_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur reconnaissance passive: {str(e)}")
            # Retourner donn√©es vides en cas d'erreur
            return PassiveReconData(
                total_assets_discovered=0,
                execution_time=time.time() - debut
            )

    async def phase_reconnaissance(
        self, url_cible: str
    ) -> DonneesReconnaissance:
        """
        Phase de reconnaissance : collecter un maximum d'informations sur la cible
        
        Args:
            url_cible: URL de la cible
            
        Returns:
            DonneesReconnaissance: Toutes les donn√©es collect√©es
        """
        debut = time.time()
        donnees = DonneesReconnaissance(url_cible=url_cible)
        
        # Ex√©cuter toutes les t√¢ches de reconnaissance en parall√®le
        taches = [
            self.enum_sousdomaines.enumerer(url_cible),
            self.scanner_ports.scanner(url_cible),
            self.detecteur_tech.detecter(url_cible),
            self.fuzzer_rep.fuzzer(url_cible)
        ]
        
        resultats = await asyncio.gather(*taches, return_exceptions=True)
        
        # Traiter les r√©sultats
        if not isinstance(resultats[0], Exception):
            donnees.sousdomaines = resultats[0]
        if not isinstance(resultats[1], Exception):
            donnees.ports_ouverts = resultats[1]
        if not isinstance(resultats[2], Exception):
            donnees.technologies = resultats[2]
        if not isinstance(resultats[3], Exception):
            donnees.repertoires = resultats[3]
        
        self.stats['temps_par_phase']['reconnaissance'] = time.time() - debut
        logger.info(
            f"Reconnaissance termin√©e: {len(donnees.sousdomaines)} sous-domaines, "
            f"{len(donnees.ports_ouverts)} ports, "
            f"{len(donnees.technologies)} technologies"
        )
        
        return donnees

    async def phase_detection_vulnerabilites(
        self, url_cible: str, donnees_recon: DonneesReconnaissance
    ) -> List[Vulnerabilite]:
        """
        Phase de d√©tection : scanner toutes les vuln√©rabilit√©s possibles
        
        Args:
            url_cible: URL de la cible
            donnees_recon: Donn√©es de la reconnaissance
            
        Returns:
            List[Vulnerabilite]: Liste des vuln√©rabilit√©s d√©tect√©es
        """
        debut = time.time()
        vulnerabilites = []
        
        # ‚≠ê LOGIQUE CIBLAGE MANUEL
        if self.scan_type == 'specific_url':
            logger.info(f"üéØ Scan cibl√© sur l'URL unique: {url_cible}")
            endpoints = [url_cible]
        else:
            # ‚≠ê AM√âLIORATION: Tester TOUS les endpoints d√©couverts, pas seulement ceux qui passent le filtre strict
            # Le filtre est moins strict maintenant, mais on teste aussi les endpoints "suspects"
            logger.info(f"üîç Filtrage des {len(donnees_recon.repertoires)} URLs d√©couvertes...")
            endpoints_existants = await self._filtrer_endpoints_existants([url_cible] + donnees_recon.repertoires)
            
            # ‚≠ê NOUVEAU: Ajouter les pages connues de testphp.vulnweb.com et autres sites vuln√©rables
            pages_connues = self._get_pages_connues(url_cible)
            endpoints_connus = [urljoin(url_cible, page) for page in pages_connues]
            
            # ‚≠ê NOUVEAU: Ajouter aussi les endpoints d√©couverts m√™me s'ils n'ont pas pass√© le filtre strict
            # (pour tester les pages qui pourraient √™tre vuln√©rables mais rejet√©es par le filtre)
            tous_endpoints = list(set(endpoints_existants + endpoints_connus + donnees_recon.repertoires[:30]))  # ‚≠ê Augment√© de 20 √† 30
            endpoints = tous_endpoints[:self.config.get('max_urls', 80)]  # ‚≠ê Augment√© de 50 √† 80
            
            logger.info(f"‚úÖ {len(endpoints)} endpoints √† tester ({len(endpoints_existants)} valid√©s + {len(tous_endpoints) - len(endpoints_existants)} suppl√©mentaires)")
        
        # ‚≠ê NOUVEAU: D√©couvrir automatiquement les param√®tres pour chaque endpoint
        logger.info(f"üîç D√©couverte automatique des param√®tres pour {len(endpoints)} endpoints...")
        parametres_par_endpoint = await self.decouvreur_params.decouvrir_pour_endpoints(
            endpoints
        )
        logger.info(f"‚úÖ Param√®tres d√©couverts pour {len(parametres_par_endpoint)} endpoints")
        
        # Cr√©er les t√¢ches de scan pour chaque type de vuln√©rabilit√©
        taches = []
        
        for endpoint in endpoints:
            # R√©cup√©rer les param√®tres d√©couverts pour cet endpoint
            params_endpoint = parametres_par_endpoint.get(endpoint, {})
            
            # ‚≠ê AM√âLIORATION: Tester plus de vuln√©rabilit√©s par endpoint (avec filtrage par modules)
            modules_vides = not self.modules_cibles or len(self.modules_cibles) == 0
            
            if modules_vides or 'sql' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.scanner_sql.scanner(endpoint, params_endpoint))
            
            if modules_vides or 'xss' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.scanner_xss.scanner(endpoint, params_endpoint))
            
            if modules_vides or 'xxe' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.detecteur_xxe.detecter(endpoint))
            
            if modules_vides or 'rce' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.chercheur_rce.chercher(endpoint, params_endpoint))
            
            if modules_vides or 'idor' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.verif_idor.verifier(endpoint))
            
            if modules_vides or 'upload' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.scanner_upload.scanner(endpoint))

            if modules_vides or 'api' in self.modules_cibles or 'all' in self.modules_cibles:
                # D√©tecter si c'est une API (JSON)
                # Pour l'instant on passe un dictionnaire vide, le fuzzer devra peut-√™tre d√©couvrir le format
                # Ou on utilise les param√®tres d√©couverts s'ils sont au format JSON (√† impl√©menter)
                taches.append(self.api_fuzzer.scanner(endpoint, method="POST", data={"test": "test"})) # Placeholder
            
            # ‚≠ê NOUVEAU: GraphQL Fuzzing
            if modules_vides or 'graphql' in self.modules_cibles or 'api' in self.modules_cibles or 'all' in self.modules_cibles:
                # Initialiser la session pour GraphQL fuzzer (utilise la session aiohttp du scanner)
                # On ne peut pas initialiser dans __init__ car la session n'existe pas encore
                if not hasattr(self.graphql_fuzzer, 'session') or self.graphql_fuzzer.session is None:
                    # On r√©cup√®re la session depuis le contexte
                    import aiohttp
                    async def _init_graphql_session():
                        timeout = aiohttp.ClientTimeout(total=30)
                        connector = aiohttp.TCPConnector(limit=20, ssl=False)
                        session = aiohttp.ClientSession(timeout=timeout, connector=connector)
                        self.graphql_fuzzer.session = session
                        return await self.graphql_fuzzer.scanner(endpoint, params_endpoint)
                    
                    taches.append(_init_graphql_session())
                else:
                    taches.append(self.graphql_fuzzer.scanner(endpoint, params_endpoint))
            
            # ‚≠ê NOUVEAUX MODULES v4.3 (combler les lacunes critiques)
            if modules_vides or 'ssrf' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.ssrf_detector.detecter(endpoint, params_endpoint))
            
            if modules_vides or 'ssti' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.ssti_scanner.scanner(endpoint, params_endpoint))
            
            if modules_vides or 'nosql' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.nosql_scanner.scanner(endpoint, params_endpoint))
            
            if modules_vides or 'deserialization' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.deserialization_detector.detecter(endpoint, params_endpoint))
            
            # ‚≠ê NOUVEAUX MODULES v4.4 (couverture compl√®te 100%)
            if modules_vides or 'prototype_pollution' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.prototype_pollution_detector.detecter(endpoint, params_endpoint))
            
            if modules_vides or 'ldap' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.ldap_injection_detector.detecter(endpoint, params_endpoint))
            
            if modules_vides or 'open_redirect' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.open_redirect_detector.detecter(endpoint, params_endpoint))
            
            if modules_vides or 'websocket' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.websocket_detector.detecter(endpoint))
            
            if modules_vides or 'race_conditions' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.race_conditions_detector.detecter(endpoint, params_endpoint))
            
            if modules_vides or 'business_logic' in self.modules_cibles or 'all' in self.modules_cibles:
                taches.append(self.business_logic_detector.detecter(endpoint, params_endpoint))
        
        # Scans globaux (avec filtrage par modules)
        modules_vides = not self.modules_cibles or len(self.modules_cibles) == 0
        
        if modules_vides or 'cors' in self.modules_cibles or 'all' in self.modules_cibles:
            taches.append(self.analyseur_cors.analyser(url_cible))
        
        if modules_vides or 'headers' in self.modules_cibles or 'all' in self.modules_cibles:
            taches.append(self.analyseur_headers.analyser(url_cible))
        
        if modules_vides or 'cve' in self.modules_cibles or 'all' in self.modules_cibles:
            taches.append(self.scanner_cve.scanner(url_cible, donnees_recon.technologies))
        
        if modules_vides or 'config' in self.modules_cibles or 'all' in self.modules_cibles:
            taches.append(self.analyseur_config.analyser(url_cible, donnees_recon.technologies))
        
        # ‚≠ê NOUVEAU v4.4: Clickjacking (scan global)
        if modules_vides or 'clickjacking' in self.modules_cibles or 'all' in self.modules_cibles:
            taches.append(self.clickjacking_detector.detecter(url_cible))
        
        if modules_vides or 'auth' in self.modules_cibles or 'all' in self.modules_cibles:
            taches.append(self.testeur_auth.tester(url_cible))
        
        if modules_vides or 'csrf' in self.modules_cibles or 'all' in self.modules_cibles:
            taches.append(self.detecteur_csrf.detecter(url_cible))
        
        # Ex√©cuter tous les scans en parall√®le avec limite de concurrence
        semaphore = asyncio.Semaphore(self.config.get('threads', 10))
        
        async def scanner_avec_limite(tache):
            # ‚≠ê NOUVEAU: V√©rifier la pause avant de lancer la t√¢che
            await self.pause_event.wait()
            
            async with semaphore:
                try:
                    # ‚≠ê OPTIMISATION: Timeout par module pour √©viter les blocages
                    # 5 minutes max par module (sauf si c'est un scan long connu)
                    return await asyncio.wait_for(tache, timeout=300)
                except asyncio.TimeoutError:
                    logger.warning("‚ö†Ô∏è  Timeout module (5min) - passage au suivant")
                    return None
                except Exception as e:
                    logger.warning(f"Erreur dans un scanner: {str(e)}")
                    return None
        
        resultats = await asyncio.gather(
            *[scanner_avec_limite(t) for t in taches],
            return_exceptions=True
        )
        
        # Collecter toutes les vuln√©rabilit√©s trouv√©es
        for resultat in resultats:
            if resultat and not isinstance(resultat, Exception):
                if isinstance(resultat, list):
                    vulnerabilites.extend(resultat)
                    # ‚≠ê NOUVEAU: Notifier le dashboard pour chaque vuln√©rabilit√© trouv√©e
                    if self.callback_vulnerabilite:
                        for v in resultat:
                            self.callback_vulnerabilite(v)
                else:
                    vulnerabilites.append(resultat)
                    # ‚≠ê NOUVEAU: Notifier le dashboard
                    if self.callback_vulnerabilite:
                        self.callback_vulnerabilite(resultat)
        
        self.stats['temps_par_phase']['detection'] = time.time() - debut
        self.stats['vulnerabilites_trouvees'] = len(vulnerabilites)
        
        logger.info(f"D√©tection termin√©e: {len(vulnerabilites)} vuln√©rabilit√©s potentielles")

        return vulnerabilites

    async def _filtrer_endpoints_existants(self, urls: List[str]) -> List[str]:
        """
        Filtre les URLs pour ne garder que celles qui existent r√©ellement
        √âvite les faux positifs sur des pages qui n'existent pas

        Version am√©lior√©e pour les apps modernes (React, SPA, etc.)
        """
        urls_existantes = []
        url_base = urls[0] if urls else ""  # URL principale pour r√©f√©rence

        # R√©cup√©rer le contenu de la page principale pour comparer
        contenu_principal = ""
        try:
            async with aiohttp.ClientSession(cookies=self.auth_config.get('cookies'), headers=self.auth_config.get('headers')) as session:
                async with session.get(url_base, timeout=aiohttp.ClientTimeout(total=5)) as response:
                    if response.status == 200:
                        contenu_principal = await response.text()
        except:
            pass

        async with aiohttp.ClientSession(cookies=self.auth_config.get('cookies'), headers=self.auth_config.get('headers')) as session:
            for url in urls:
                # L'URL principale existe toujours
                if url == url_base:
                    urls_existantes.append(url)
                    logger.debug(f"‚úÖ URL principale conserv√©e: {url}")
                    continue

                try:
                    async with session.get(
                        url,
                        timeout=aiohttp.ClientTimeout(total=3),
                        allow_redirects=False
                    ) as response:
                        if response.status == 200:
                            contenu = await response.text()
                            contenu_lower = contenu.lower()

                            # ‚≠ê FILTRE AM√âLIOR√â : Moins strict pour accepter plus de pages
                            # 1. D√©tection des pages d'erreur classiques (patterns plus sp√©cifiques)
                            indicateurs_erreur_stricts = [
                                '404 not found', 'page not found', 'error 404',
                                'file not found', 'does not exist',
                                'cannot find the page', 'the page you requested',
                                'page unavailable', 'resource not found',
                                'document not found', 'requested url was not found'
                            ]
                            
                            # Ne pas rejeter si juste "error" ou "not found" (trop g√©n√©rique)
                            contient_erreur = any(
                                indicateur in contenu_lower 
                                for indicateur in indicateurs_erreur_stricts
                            ) and (
                                '404' in contenu_lower or 
                                'not found' in contenu_lower or
                                len(contenu.strip()) < 200  # Contenu tr√®s court = probable erreur
                            )

                            # 2. D√©tection des Single Page Applications (SPA)
                            # Si le contenu est identique √† la page principale, c'est probablement du routing c√¥t√© client
                            contenu_identique_principal = (
                                contenu_principal and
                                contenu.strip() == contenu_principal.strip() and
                                len(contenu.strip()) > 500  # Contenu substantiel
                            )

                            # 3. V√©rifier si c'est une vraie API ou page
                            # Les vraies pages/API ont g√©n√©ralement du contenu diff√©rent ou sp√©cifique
                            est_api_endpoint = any(pattern in url.lower() for pattern in [
                                '/api/', '/rest/', '/graphql', '/v1/', '/v2/', '/v3/',
                                '.json', '.xml', '/data/', '/endpoint'
                            ])

                            # 4. Contenu trop court = probablement une erreur (seuil plus bas)
                            contenu_trop_court = len(contenu.strip()) < 30  # R√©duit de 50 √† 30

                            # ‚≠ê LOGIQUE AM√âLIOR√âE : Accepter plus de pages
                            # Accepter si :
                            # - Pas d'erreur claire OU
                            # - Contenu diff√©rent de la page principale OU
                            # - C'est une API endpoint OU
                            # - Contenu substantiel (>30 caract√®res)
                            
                            if contient_erreur and len(contenu.strip()) < 200:
                                logger.debug(f"‚ùå Page d'erreur exclue: {url}")
                            elif contenu_identique_principal and not est_api_endpoint and len(contenu.strip()) > 1000:
                                # Seulement exclure si contenu identique ET tr√®s long (SPA probable)
                                logger.debug(f"‚ùå SPA routing exclu: {url}")
                            elif contenu_trop_court:
                                logger.debug(f"‚ùå Contenu trop court exclu: {url}")
                            else:
                                urls_existantes.append(url)
                                logger.debug(f"‚úÖ URL existante valid√©e: {url}")

                        elif response.status in [301, 302, 303, 307, 308]:
                            # ‚≠ê NOUVEAU: Accepter les redirections (peuvent √™tre vuln√©rables)
                            urls_existantes.append(url)
                            logger.debug(f"‚úÖ Redirection accept√©e: {url} (status {response.status})")
                        elif response.status == 403:
                            # ‚≠ê NOUVEAU: Accepter les 403 (peuvent indiquer des endpoints existants prot√©g√©s)
                            urls_existantes.append(url)
                            logger.debug(f"‚úÖ 403 accept√© (endpoint prot√©g√©): {url}")
                        else:
                            logger.debug(f"‚ùå URL non accessible ({response.status}): {url}")

                except Exception as e:
                    logger.debug(f"‚ùå Erreur v√©rification {url}: {str(e)}")

        # Log final
        logger.info(f"üìä Filtrage termin√©: {len(urls)} URLs test√©es ‚Üí {len(urls_existantes)} URLs valides")
        return urls_existantes

    def _get_pages_connues(self, url_base: str) -> List[str]:
        """
        Retourne une liste de pages connues √† tester pour les sites vuln√©rables courants
        (testphp.vulnweb.com, etc.)
        
        Args:
            url_base: URL de base du site
            
        Returns:
            List[str]: Liste de chemins relatifs √† tester
        """
        # Pages communes sur testphp.vulnweb.com et sites similaires
        pages_communes = [
            # Pages principales avec param√®tres
            'artists.php',
            'listproducts.php',
            'listart.php',
            'showproduct.php',
            'product.php',
            'categories.php',
            'category.php',
            'search.php',
            'comment.php',
            'comments.php',
            'guestbook.php',
            'contact.php',
            'gallery.php',
            'pictures.php',
            'showimage.php',
            # Upload
            'upload.php',
            'fileupload.php',
            'upload_file.php',
            # Auth
            'login.php',
            'register.php',
            'signup.php',
            'signin.php',
            # Admin
            'admin.php',
            'admin',
            'dashboard.php',
            # API
            'api.php',
            'api',
            # Autres
            'user.php',
            'users.php',
            'profile.php',
            'account.php',
        ]
        
        return pages_communes

    async def phase_validation(
        self, vulnerabilites: List[Vulnerabilite]
    ) -> List[Vulnerabilite]:
        """
        Phase de validation : √©liminer les faux positifs
        
        Args:
            vulnerabilites: Liste des vuln√©rabilit√©s √† valider
            
        Returns:
            List[Vulnerabilite]: Vuln√©rabilit√©s valid√©es
        """
        debut = time.time()
        vulns_validees = []
        faux_positifs = 0
        
        # Validation avec tests d'exploitation r√©els (toujours activ√©e)
        logger.info(f"üîç Validation de {len(vulnerabilites)} vuln√©rabilit√©s avec tests d'exploitation...")
        
        taches_validation = [
            self.validateur.valider(vuln) for vuln in vulnerabilites
        ]
        
        resultats = await asyncio.gather(*taches_validation, return_exceptions=True)
        
        for vuln, est_valide in zip(vulnerabilites, resultats):
            if not isinstance(est_valide, Exception) and est_valide:
                vuln.validee = True
                vulns_validees.append(vuln)
                logger.success(f"‚úÖ {vuln.type} confirm√©e comme exploitable")
            else:
                faux_positifs += 1
                logger.warning(f"‚ùå {vuln.type} rejet√©e (faux positif ou non exploitable)")
        
        self.stats['temps_par_phase']['validation'] = time.time() - debut
        self.stats['faux_positifs_elimines'] = faux_positifs
        
        # D√©dupliquer les vuln√©rabilit√©s identiques
        vulns_uniques = self._dedupliquer_vulnerabilites(vulns_validees)
        nb_duplications = len(vulns_validees) - len(vulns_uniques)
        
        if nb_duplications > 0:
            logger.info(f"üîÑ {nb_duplications} duplication(s) √©limin√©e(s)")
        
        logger.info(
            f"Validation termin√©e: {len(vulns_uniques)} confirm√©es, "
            f"{faux_positifs} faux positifs √©limin√©s"
        )
        
        return vulns_uniques

    async def phase_generation_exploits(
        self, vulnerabilites: List[Vulnerabilite]
    ) -> None:
        """
        Phase de g√©n√©ration d'exploits avec l'IA
        
        Args:
            vulnerabilites: Liste des vuln√©rabilit√©s valid√©es
        """
        debut = time.time()
        
        taches = [
            self.generateur_exploits.generer(vuln) for vuln in vulnerabilites
        ]
        
        exploits = await asyncio.gather(*taches, return_exceptions=True)
        
        for vuln, exploit in zip(vulnerabilites, exploits):
            if exploit and not isinstance(exploit, Exception):
                vuln.exploit_disponible = True
                vuln.exploit_code = exploit
        
        self.stats['temps_par_phase']['generation_exploits'] = time.time() - debut
        logger.info("Exploits g√©n√©r√©s avec succ√®s")

    async def phase_chaines_exploit(
        self, vulnerabilites: List[Vulnerabilite]
    ) -> List[Dict]:
        """
        Phase de construction de cha√Ænes d'exploit
        
        Args:
            vulnerabilites: Liste des vuln√©rabilit√©s
            
        Returns:
            List[Dict]: Cha√Ænes d'exploit possibles
        """
        debut = time.time()
        
        chaines = await self.constructeur_chaines.construire_chaines(
            vulnerabilites
        )
        
        self.stats['temps_par_phase']['chaines_exploit'] = time.time() - debut
        logger.info(f"{len(chaines)} cha√Ænes d'exploit identifi√©es")
        
        return chaines

    def calculer_score_risque_global(
        self, vulnerabilites: List[Vulnerabilite]
    ) -> float:
        """
        Calcule un score de risque global intelligent avec ML

        Args:
            vulnerabilites: Liste des vuln√©rabilit√©s

        Returns:
            float: Score de risque entre 0 et 10
        """
        try:
            # Utiliser le syst√®me ML de scoring intelligent
            contexte = {
                'production': True,  # Par d√©faut on consid√®re production
                'internet_facing': True
            }

            # Pour l'instant on n'a pas les anomalies, mais on peut les ajouter plus tard
            anomalies = []

            # Calculer le score avec ML
            resultats_scoring = self.scorer_risque.calculer_score_global(
                vulnerabilites,
                getattr(self, '_technologies_detectees', {}),
                contexte,
                anomalies
            )

            return resultats_scoring.get('score_global', 5.0)

        except Exception as e:
            logger.debug(f"Erreur scoring ML, fallback simple: {str(e)}")

            # Fallback vers le calcul simple en cas d'erreur
            if not vulnerabilites:
                return 0.0

            scores_severite = {
                'CRITIQUE': 10.0, '√âLEV√â': 7.5, 'MOYEN': 5.0,
                'FAIBLE': 2.5, 'INFO': 0.5
            }

            score_total = sum(
                vuln.cvss_score or scores_severite.get(vuln.severite, 0)
                for vuln in vulnerabilites
            )

            score_moyen = score_total / len(vulnerabilites)

            # Bonus si beaucoup de vuln√©rabilit√©s critiques
            critiques = sum(1 for v in vulnerabilites if v.severite == 'CRITIQUE')
            bonus = min(critiques * 0.5, 2.0)

            return min(score_moyen + bonus, 10.0)

    def _dedupliquer_vulnerabilites(self, vulnerabilites: List[Vulnerabilite]) -> List[Vulnerabilite]:
        """
        √âlimine les vuln√©rabilit√©s en double bas√©es sur type + URL + param√®tre
        
        Une m√™me faille SQL avec 20 payloads diff√©rents = 1 seule vuln√©rabilit√©
        Pour les vuln√©rabilit√©s FAIBLES/INFO (ex: headers manquants): grouper par type seulement
        
        Args:
            vulnerabilites: Liste des vuln√©rabilit√©s
            
        Returns:
            List[Vulnerabilite]: Liste d√©dupliqu√©e
        """
        from urllib.parse import urlparse, parse_qs
        
        vues = {}  # Dict pour garder la meilleure vuln√©rabilit√© par type+page
        params_par_vuln = {}  # ‚≠ê NOUVEAU: Collecter les param√®tres affect√©s par type+page
        
        for vuln in vulnerabilites:
            # Extraire l'URL de base (sans query string)
            parsed = urlparse(vuln.url)
            base_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            
            # ‚≠ê NOUVEAU: Cl√© = TYPE + PAGE (ignorer le param√®tre pour grouper)
            # Exemple: "XSS:/search.php" regroupe tous les XSS de cette page
            cle = f"{vuln.type}:{base_url}"
            
            # Extraire le param√®tre pour le comptage
            parametre = self._extraire_parametre_vulnerable(vuln)
            
            if cle not in vues:
                # Premi√®re occurrence de ce type sur cette page
                vues[cle] = vuln
                params_par_vuln[cle] = {parametre} if parametre != 'unknown' else set()
                logger.debug(f"‚úÖ Nouvelle vuln√©rabilit√©: {vuln.type} sur {base_url}")
            else:
                # Ajouter le param√®tre √† la liste des param√®tres affect√©s
                if parametre != 'unknown':
                    params_par_vuln[cle].add(parametre)
                
                # Garder la vuln√©rabilit√© avec le meilleur score CVSS ou la meilleure preuve
                vuln_existante = vues[cle]
                if vuln.cvss_score > vuln_existante.cvss_score:
                    vues[cle] = vuln
                elif self._est_meilleure_preuve(vuln, vuln_existante):
                    vues[cle] = vuln
                
                logger.debug(f"üîÑ Group√©: {vuln.type} sur {base_url} (param√®tre: {parametre})")
        
        # ‚≠ê NOUVEAU: Mettre √† jour les descriptions avec le nombre de param√®tres affect√©s
        for cle, vuln in vues.items():
            params = params_par_vuln.get(cle, set())
            if len(params) > 1:
                vuln.description = f"{vuln.description} (affecte {len(params)} param√®tres: {', '.join(list(params)[:5])}{'...' if len(params) > 5 else ''})"
        
        vulns_uniques = list(vues.values())
        
        logger.info(f"üìä D√©duplication: {len(vulnerabilites)} ‚Üí {len(vulns_uniques)} vuln√©rabilit√©s uniques")
        
        return vulns_uniques
    
    def _extraire_parametre_vulnerable(self, vuln: Vulnerabilite) -> str:
        """
        Extrait le nom du param√®tre vuln√©rable depuis la description
        
        Args:
            vuln: Vuln√©rabilit√©
            
        Returns:
            str: Nom du param√®tre ou 'unknown'
        """
        import re
        
        # Chercher "param√®tre 'xxx'" dans la description
        match = re.search(r"param√®tre\s+'([^']+)'", vuln.description, re.IGNORECASE)
        if match:
            return match.group(1)
        
        # Chercher "dans 'xxx'" dans la description
        match = re.search(r"dans\s+'([^']+)'", vuln.description, re.IGNORECASE)
        if match:
            return match.group(1)
        
        # Extraire depuis l'URL si possible
        from urllib.parse import urlparse, parse_qs
        parsed = urlparse(vuln.url)
        params = parse_qs(parsed.query)
        if params:
            # Retourner le premier param√®tre
            return list(params.keys())[0]
        
        # Par d√©faut, utiliser le type + partie de l'URL pour diff√©rencier
        return parsed.path.split('/')[-1] or 'root'
    
    def _est_meilleure_preuve(self, nouvelle: Vulnerabilite, existante: Vulnerabilite) -> bool:
        """
        D√©termine si une nouvelle vuln√©rabilit√© a une meilleure preuve que l'existante
        
        Args:
            nouvelle: Nouvelle vuln√©rabilit√©
            existante: Vuln√©rabilit√© existante
            
        Returns:
            bool: True si la nouvelle est meilleure
        """
        # Ordre de priorit√© pour les preuves SQL Injection
        priorites = {
            'UNION SELECT': 10,  # Le meilleur
            'r√©ussie avec': 9,
            'colonnes': 8,
            'SQL syntax': 5,     # Erreur basique
            'error': 4,
            'syntax': 3,
            'database': 2,
        }
        
        score_nouvelle = 0
        score_existante = 0
        
        # Calculer le score de la nouvelle
        for mot_cle, points in priorites.items():
            if mot_cle.lower() in (nouvelle.preuve or '').lower():
                score_nouvelle = max(score_nouvelle, points)
            if mot_cle.lower() in (nouvelle.description or '').lower():
                score_nouvelle = max(score_nouvelle, points - 1)
        
        # Calculer le score de l'existante
        for mot_cle, points in priorites.items():
            if mot_cle.lower() in (existante.preuve or '').lower():
                score_existante = max(score_existante, points)
            if mot_cle.lower() in (existante.description or '').lower():
                score_existante = max(score_existante, points - 1)
        
        # La nouvelle est meilleure si elle a un score sup√©rieur
        return score_nouvelle > score_existante


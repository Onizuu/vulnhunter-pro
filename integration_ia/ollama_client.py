"""
Client Ollama pour g√©n√©ration de payloads et analyse (IA locale gratuite)
"""

import os
import json
import re
import asyncio
from typing import Optional, Dict, List, Union
from loguru import logger
import aiohttp


class ClientOllama:
    """
    Client pour Ollama (IA locale gratuite)
    Utilise des mod√®les locaux comme Mistral 7B, CodeLlama, etc.
    """

    def __init__(self, base_url: str = "http://localhost:11434", model: str = "mistral:7b"):
        """
        Initialise le client Ollama
        
        Args:
            base_url: URL de l'API Ollama (par d√©faut localhost:11434)
            model: Mod√®le √† utiliser (mistral:7b recommand√© pour cybers√©curit√©)
        """
        self.base_url = base_url
        self.model = model
        self.disponible = False
        
        # Tester la connexion
        try:
            import requests
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                # V√©rifier que le mod√®le est disponible
                models = response.json().get('models', [])
                model_names = [m.get('name', '') for m in models]
                
                if any(model in name for name in model_names):
                    self.disponible = True
                    logger.info(f"‚úÖ Client Ollama initialis√© (mod√®le: {model})")
                else:
                    logger.warning(f"‚ö†Ô∏è  Mod√®le {model} non trouv√© dans Ollama")
                    logger.info(f"üí° Mod√®les disponibles: {', '.join(model_names[:5]) if model_names else 'Aucun'}")
                    logger.info(f"üí° Installez le mod√®le: ollama pull {model}")
                    # Essayer quand m√™me (le mod√®le pourrait √™tre charg√© dynamiquement)
                    self.disponible = True
                    logger.warning("‚ö†Ô∏è  Tentative de connexion malgr√© mod√®le non trouv√©")
            else:
                logger.warning(f"‚ö†Ô∏è  Ollama non accessible (status: {response.status_code})")
        except requests.exceptions.ConnectionError as e:
            logger.warning(f"‚ö†Ô∏è  Ollama non accessible: {str(e)}")
            logger.info("üí° V√©rifiez que Ollama est d√©marr√©: ollama serve")
        except requests.exceptions.Timeout:
            logger.warning("‚ö†Ô∏è  Timeout lors de la connexion √† Ollama")
        except ImportError:
            logger.warning("‚ö†Ô∏è  Biblioth√®que 'requests' non disponible pour test Ollama")
            # Essayer quand m√™me avec aiohttp
            self.disponible = True
            logger.info(f"‚úÖ Client Ollama initialis√© (mod√®le: {model}) - Test de connexion diff√©r√©")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Ollama non disponible: {type(e).__name__}: {str(e)}")
            logger.info("üí° Installez Ollama: brew install ollama")
            logger.info(f"üí° Puis: ollama serve && ollama pull {model}")
        
        self.temperature = 0.7
        self.max_tokens = 4000

    async def generer_completion(
        self,
        prompt: str,
        json_mode: bool = False,
        temperature: Optional[float] = None
    ) -> Optional[Union[str, Dict]]:
        """
        G√©n√®re une completion avec Ollama
        
        Args:
            prompt: Le prompt √† envoyer
            json_mode: Si True, force la r√©ponse en JSON
            temperature: Temp√©rature de g√©n√©ration
            
        Returns:
            str|Dict: R√©ponse g√©n√©r√©e ou None si erreur
        """
        if not self.disponible:
            logger.warning("Client Ollama non disponible")
            return None
        
        try:
            url = f"{self.base_url}/api/generate"
            
            # Am√©liorer le prompt pour JSON si n√©cessaire
            if json_mode:
                prompt = f"""{prompt}

IMPORTANT: R√©ponds UNIQUEMENT avec un JSON valide, sans texte avant ou apr√®s.
Format JSON attendu: {{"payloads": ["payload1", "payload2", ...]}}"""
            
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": temperature or self.temperature,
                    "num_predict": self.max_tokens
                }
            }
            
            async with aiohttp.ClientSession() as session:
                try:
                    # ‚≠ê Timeout augment√© : laisser Ollama prendre son temps (local = gratuit)
                    timeout_seconds = 90  # 90s pour laisser l'IA locale travailler
                    async with session.post(
                        url,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=timeout_seconds)
                    ) as resp:
                        if resp.status == 200:
                            try:
                                data = await resp.json()
                            except Exception as je:
                                error_text = await resp.text()
                                logger.error(f"Erreur parsing JSON r√©ponse Ollama: {type(je).__name__}: {str(je)}")
                                logger.debug(f"R√©ponse brute (200 premiers chars): {error_text[:200]}")
                                return None
                            
                            contenu = data.get('response', '').strip()
                            
                            if not contenu:
                                logger.warning("R√©ponse Ollama vide (pas de contenu dans 'response')")
                                logger.debug(f"Donn√©es re√ßues: {list(data.keys())}")
                                return None
                            
                            if json_mode:
                                try:
                                    return json.loads(contenu)
                                except json.JSONDecodeError:
                                    # Essayer d'extraire le JSON de la r√©ponse
                                    json_match = re.search(r'\{.*\}', contenu, re.DOTALL)
                                    if json_match:
                                        try:
                                            return json.loads(json_match.group())
                                        except json.JSONDecodeError as je:
                                            logger.debug(f"JSON invalide extrait: {str(je)}")
                                    logger.debug(f"R√©ponse JSON invalide de Ollama (premiers 200 chars): {contenu[:200]}")
                                    return None
                            
                            return contenu
                        else:
                            error_text = await resp.text()
                            logger.error(f"Erreur HTTP Ollama: {resp.status} - {error_text[:200]}")
                            return None
                except aiohttp.ClientConnectorError as e:
                    error_msg = str(e) if str(e) else f"Connexion refus√©e √† {self.base_url}"
                    logger.error(f"Impossible de se connecter √† Ollama ({self.base_url}): {error_msg}")
                    logger.info("üí° V√©rifiez que Ollama est d√©marr√©: ollama serve")
                    return None
                except asyncio.TimeoutError:
                    logger.warning(f"Timeout Ollama ({timeout_seconds}s) - R√©ponse tr√®s lente, utilisation de payloads de base")
                    logger.debug(f"D√©tails: URL={url}, Model={self.model}, Prompt length={len(prompt)}")
                    return None  # Retourner None pour utiliser les payloads de base
                except aiohttp.ClientError as e:
                    error_msg = str(e) if str(e) else f"Erreur client HTTP {type(e).__name__}"
                    logger.error(f"Erreur connexion Ollama: {type(e).__name__}: {error_msg}")
                    logger.debug(f"D√©tails: URL={url}, Model={self.model}")
                    if hasattr(e, 'message'):
                        logger.debug(f"Message exception: {e.message}")
                    if hasattr(e, 'status'):
                        logger.debug(f"Status: {e.status}")
                    return None
                        
        except json.JSONDecodeError as e:
            logger.error(f"Erreur parsing JSON Ollama: {str(e)}")
            return None
        except Exception as e:
            error_msg = str(e) if str(e) else f"Exception {type(e).__name__} sans message"
            logger.error(f"Erreur inattendue Ollama: {type(e).__name__}: {error_msg}")
            import traceback
            logger.debug(f"Traceback complet:\n{traceback.format_exc()}")
            # Si le message est vide, afficher plus d'infos
            if not str(e):
                logger.error(f"D√©tails exception: {repr(e)}")
                logger.error(f"Attributs: {dir(e)}")
            return None

    async def generer_payloads_sqli(
        self,
        contexte: str,
        dbms: Optional[str] = None,
        filtres: Optional[List[str]] = None
    ) -> List[str]:
        """
        G√©n√®re des payloads d'injection SQL personnalis√©s
        
        Args:
            contexte: Contexte de l'injection
            dbms: Type de SGBD (MySQL, PostgreSQL, etc.)
            filtres: Filtres WAF d√©tect√©s
            
        Returns:
            List[str]: Payloads g√©n√©r√©s
        """
        dbms_str = dbms or "inconnu"
        filtres_str = ", ".join(filtres) if filtres else "aucun"
        
        prompt = f"""G√©n√®re 30 payloads d'injection SQL avanc√©s pour contourner les WAF modernes.

Contexte: {contexte}
SGBD: {dbms_str}
Filtres WAF: {filtres_str}

Les payloads doivent:
1. Contourner les filtres WAF (Cloudflare, AWS WAF, ModSecurity)
2. Utiliser des techniques d'obfuscation vari√©es
3. Inclure des payloads temporels et bas√©s sur erreur
4. Tester l'extraction de donn√©es
5. √ätre fonctionnels

Techniques √† utiliser:
- Encodage multiple (URL, hex, unicode)
- Commentaires SQL vari√©s
- Variations de casse
- Espaces alternatifs
- Op√©rateurs logiques alternatifs
- Fonctions SQL alternatives

Retourne uniquement un JSON:
{{
    "payloads": ["payload1", "payload2", ...]
}}"""

        resultat = await self.generer_completion(prompt, json_mode=True)
        
        if resultat and 'payloads' in resultat:
            return resultat['payloads']
        
        return []

    async def generer_payloads_xss(
        self,
        contexte: str,
        filtres: Optional[List[str]] = None
    ) -> List[str]:
        """
        G√©n√®re des payloads XSS personnalis√©s
        
        Args:
            contexte: Contexte (HTML, JavaScript, attribut, etc.)
            filtres: Filtres d√©tect√©s
            
        Returns:
            List[str]: Payloads XSS
        """
        filtres_str = ", ".join(filtres) if filtres else "aucun"
        
        prompt = f"""G√©n√®re 30 payloads XSS innovants pour contourner les filtres modernes.

Contexte: {contexte}
Filtres: {filtres_str}

Les payloads doivent:
1. Contourner CSP (Content Security Policy)
2. Fonctionner dans diff√©rents contextes
3. Utiliser l'obfuscation avanc√©e
4. √âviter les mots-cl√©s courants bloqu√©s
5. Inclure des variantes DOM-based

Techniques:
- Encodages multiples
- Event handlers alternatifs
- Vecteurs sans parenth√®ses
- Template literals
- Payloads polyglot
- Mutation XSS

Retourne uniquement un JSON:
{{
    "payloads": ["payload1", "payload2", ...]
}}"""

        resultat = await self.generer_completion(prompt, json_mode=True)
        
        if resultat and 'payloads' in resultat:
            return resultat['payloads']
        
        return []

    async def analyser_reponse_pour_vuln(
        self,
        url: str,
        requete: str,
        reponse: str,
        headers: Dict[str, str]
    ) -> Dict:
        """
        Analyse une r√©ponse HTTP pour d√©tecter des vuln√©rabilit√©s
        
        Args:
            url: URL test√©e
            requete: Requ√™te envoy√©e
            reponse: R√©ponse re√ßue
            headers: Headers de la r√©ponse
            
        Returns:
            Dict: Analyse des vuln√©rabilit√©s
        """
        # Tronquer pour rester dans les limites
        reponse_tronquee = reponse[:3000]
        
        prompt = f"""Analyse cette interaction HTTP et identifie toute vuln√©rabilit√© de s√©curit√©.

URL: {url}

Requ√™te:
{requete[:500]}

R√©ponse (premiers 3000 caract√®res):
{reponse_tronquee}

Headers:
{json.dumps(headers, indent=2)}

Identifie:
1. Injections SQL (messages d'erreur, comportement anormal)
2. XSS (r√©flexion non √©chapp√©e)
3. Fuites d'informations
4. Erreurs r√©v√©latrices
5. Configuration non s√©curis√©e
6. Headers de s√©curit√© manquants

Retourne un JSON:
{{
    "vulnerabilites_detectees": [
        {{
            "type": "type de vuln",
            "confiance": "haute|moyenne|faible",
            "preuve": "extrait prouvant la vuln",
            "description": "description d√©taill√©e"
        }}
    ],
    "recommandations": ["rec1", "rec2"]
}}"""

        resultat = await self.generer_completion(prompt, json_mode=True)
        
        return resultat or {"vulnerabilites_detectees": [], "recommandations": []}

    async def generer_rapport_executif(
        self,
        vulnerabilites: List,
        statistiques: Dict
    ) -> str:
        """
        G√©n√®re un r√©sum√© ex√©cutif pour le rapport
        
        Args:
            vulnerabilites: Liste des vuln√©rabilit√©s
            statistiques: Statistiques du scan
            
        Returns:
            str: R√©sum√© ex√©cutif en markdown
        """
        # R√©sumer les vuln√©rabilit√©s
        resume_vulns = []
        for v in vulnerabilites[:20]:  # Limiter √† 20
            resume_vulns.append({
                'type': v.type,
                'severite': v.severite,
                'url': v.url
            })
        
        prompt = f"""G√©n√®re un r√©sum√© ex√©cutif professionnel pour ce rapport de s√©curit√©.

Statistiques:
{json.dumps(statistiques, indent=2, ensure_ascii=False)}

Vuln√©rabilit√©s (√©chantillon):
{json.dumps(resume_vulns, indent=2, ensure_ascii=False)}

Le r√©sum√© doit:
1. √ätre compr√©hensible pour des non-techniques
2. Mettre en avant les risques business
3. Prioriser les actions √† prendre
4. √ätre concis (300-500 mots)
5. Utiliser un ton professionnel

Format: Markdown avec sections:
- Vue d'ensemble
- Risques principaux
- Recommandations prioritaires
- Prochaines √©tapes"""

        resume = await self.generer_completion(prompt)
        
        return resume or "Erreur lors de la g√©n√©ration du r√©sum√©"

    def set_modele(self, modele: str):
        """
        Change le mod√®le Ollama utilis√©
        
        Args:
            modele: Nom du mod√®le (mistral:7b, codellama:7b, etc.)
        """
        self.model = modele
        logger.info(f"Mod√®le Ollama chang√© pour: {modele}")

    def set_temperature(self, temperature: float):
        """
        Change la temp√©rature de g√©n√©ration
        
        Args:
            temperature: Valeur entre 0 et 2
        """
        self.temperature = max(0.0, min(2.0, temperature))
        logger.info(f"Temp√©rature Ollama r√©gl√©e √†: {self.temperature}")

